{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoveNet Pose Estimation Adaption with Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original MoveNet with Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom implementation of MoveNet adapting from MoveNet codebase.\n",
    "\n",
    "Refactoring helper functions, adding functions for dataframe conversion.\n",
    "\n",
    "Restructured inference functions to load video files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preping functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in ./MoveNet/lib/python3.9/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.17.0 in ./MoveNet/lib/python3.9/site-packages (from opencv-python) (1.26.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/nick/Documents/GitHub/MoveNet/MoveNet/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in ./MoveNet/lib/python3.9/site-packages (2.17.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (3.5.0)\n",
      "Requirement already satisfied: packaging in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: six>=1.12.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (1.66.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: setuptools in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (56.0.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./MoveNet/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in ./MoveNet/lib/python3.9/site-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
      "Requirement already satisfied: namex in ./MoveNet/lib/python3.9/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in ./MoveNet/lib/python3.9/site-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./MoveNet/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./MoveNet/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./MoveNet/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./MoveNet/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./MoveNet/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./MoveNet/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./MoveNet/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./MoveNet/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.18,>=2.17->tensorflow) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./MoveNet/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.18,>=2.17->tensorflow) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./MoveNet/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./MoveNet/lib/python3.9/site-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./MoveNet/lib/python3.9/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./MoveNet/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/nick/Documents/GitHub/MoveNet/MoveNet/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow-hub in ./MoveNet/lib/python3.9/site-packages (0.16.1)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in ./MoveNet/lib/python3.9/site-packages (from tensorflow-hub) (4.25.5)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow-hub) (1.26.4)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow-hub) (2.17.0)\n",
      "Requirement already satisfied: tensorflow<2.18,>=2.17 in ./MoveNet/lib/python3.9/site-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.17.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (1.66.2)\n",
      "Requirement already satisfied: six>=1.12.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (4.12.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.5.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (24.3.25)\n",
      "Requirement already satisfied: packaging in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (24.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
      "Requirement already satisfied: setuptools in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (56.0.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.6.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.4.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.17.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.32.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./MoveNet/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.44.0)\n",
      "Requirement already satisfied: optree in ./MoveNet/lib/python3.9/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.12.1)\n",
      "Requirement already satisfied: rich in ./MoveNet/lib/python3.9/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (13.8.1)\n",
      "Requirement already satisfied: namex in ./MoveNet/lib/python3.9/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.0.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./MoveNet/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./MoveNet/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./MoveNet/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./MoveNet/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./MoveNet/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./MoveNet/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./MoveNet/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./MoveNet/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./MoveNet/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./MoveNet/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.1.5)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./MoveNet/lib/python3.9/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./MoveNet/lib/python3.9/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./MoveNet/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/nick/Documents/GitHub/MoveNet/MoveNet/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in ./MoveNet/lib/python3.9/site-packages (1.26.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/nick/Documents/GitHub/MoveNet/MoveNet/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in ./MoveNet/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./MoveNet/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./MoveNet/lib/python3.9/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./MoveNet/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./MoveNet/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./MoveNet/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/nick/Documents/GitHub/MoveNet/MoveNet/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python\n",
    "%pip install tensorflow\n",
    "%pip install tensorflow-hub\n",
    "%pip install numpy\n",
    "%pip install pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "# Import matplotlib libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.3):\n",
    "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "  Args:\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    height: height of the image in pixels.\n",
    "    width: width of the image in pixels.\n",
    "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "      visualized.\n",
    "\n",
    "  Returns:\n",
    "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "      * the coordinates of all keypoints of all detected entities;\n",
    "      * the coordinates of all skeleton edges of all detected entities;\n",
    "      * the colors in which the edges should be plotted.\n",
    "  \"\"\"\n",
    "  keypoints_all = []\n",
    "  keypoint_edges_all = []\n",
    "  edge_colors = []\n",
    "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "  for idx in range(num_instances):\n",
    "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "    kpts_absolute_xy = np.stack(\n",
    "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "        kpts_scores > keypoint_threshold, :]\n",
    "    keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "        keypoint_edges_all.append(line_seg)\n",
    "        edge_colors.append(color)\n",
    "  if keypoints_all:\n",
    "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "  else:\n",
    "    keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "  if keypoint_edges_all:\n",
    "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "  else:\n",
    "    edges_xy = np.zeros((0, 2, 2))\n",
    "  return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "  \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "  Args:\n",
    "    image: A numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "      of the crop region in normalized coordinates (see the init_crop_region\n",
    "      function below for more detail). If provided, this function will also\n",
    "      draw the bounding box on the image.\n",
    "    output_image_height: An integer indicating the height of the output image.\n",
    "      Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "  \"\"\"\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  # To remove the huge white borders\n",
    "  fig.tight_layout(pad=0)\n",
    "  ax.margins(0)\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_xticklabels([])\n",
    "  plt.axis('off')\n",
    "\n",
    "  im = ax.imshow(image)\n",
    "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "  ax.add_collection(line_segments)\n",
    "  # Turn off tick labels\n",
    "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "  (keypoint_locs, keypoint_edges,\n",
    "   edge_colors) = _keypoints_and_edges_for_display(\n",
    "       keypoints_with_scores, height, width)\n",
    "\n",
    "  line_segments.set_segments(keypoint_edges)\n",
    "  line_segments.set_color(edge_colors)\n",
    "  if keypoint_edges.shape[0]:\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "  if keypoint_locs.shape[0]:\n",
    "    scat.set_offsets(keypoint_locs)\n",
    "\n",
    "  # if crop_region is not None:\n",
    "  #   xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "  #   ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "  #   rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "  #   rec_height = min(crop_region['y_max'], 0.99) * height - ymin \n",
    "  #   rect = patches.Rectangle(\n",
    "  #     (xmin,ymin),rec_width,rec_height,\n",
    "  #     linewidth=2,edgecolor='g',facecolor='none')    \n",
    "  #   ax.add_patch(rect)\n",
    "\n",
    "  fig.canvas.draw()\n",
    "  image_from_plot = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)\n",
    "  image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
    "  plt.close(fig)\n",
    "  if output_image_height is not None:\n",
    "    output_image_width = int(output_image_height / height * width)\n",
    "    image_from_plot = cv2.resize(\n",
    "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "        interpolation=cv2.INTER_CUBIC)\n",
    "  return image_from_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading MoveNet - Single Pose Thunder 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-03 16:15:34.822508: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-10-03 16:15:34.822535: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-10-03 16:15:34.822544: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-10-03 16:15:34.822560: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-10-03 16:15:34.822570: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Load the MoveNet model from TensorFlow Hub\n",
    "try:\n",
    "    module = hub.load(\"https://www.kaggle.com/models/google/movenet/TensorFlow2/singlepose-thunder/4\")\n",
    "    input_size = 256\n",
    "except Exception as error:\n",
    "    print(f\"Failed to load the model: {error}\")\n",
    "\n",
    "def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "        input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "        A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "        coordinates and scores.\n",
    "    \"\"\"\n",
    "    model = module.signatures['serving_default']\n",
    "\n",
    "    # SavedModel format expects tensor type of int32.\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    # Run model inference.\n",
    "    outputs = model(input_image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoints_with_scores = outputs['output_0'].numpy()\n",
    "    \n",
    "    return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video (Image Sequence) Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cropping Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence score to determine whether a keypoint prediction is reliable.\n",
    "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
    "\n",
    "def init_crop_region(image_height, image_width):\n",
    "  \"\"\"Defines the default crop region.\n",
    "\n",
    "  The function provides the initial crop region (pads the full image from both\n",
    "  sides to make it a square image) when the algorithm cannot reliably determine\n",
    "  the crop region from the previous frame.\n",
    "  \"\"\"\n",
    "  if image_width > image_height:\n",
    "    box_height = image_width / image_height\n",
    "    box_width = 1.0\n",
    "    y_min = (image_height / 2 - image_width / 2) / image_height\n",
    "    x_min = 0.0\n",
    "  else:\n",
    "    box_height = 1.0\n",
    "    box_width = image_height / image_width\n",
    "    y_min = 0.0\n",
    "    x_min = (image_width / 2 - image_height / 2) / image_width\n",
    "\n",
    "  return {\n",
    "    'y_min': y_min,\n",
    "    'x_min': x_min,\n",
    "    'y_max': y_min + box_height,\n",
    "    'x_max': x_min + box_width,\n",
    "    'height': box_height,\n",
    "    'width': box_width\n",
    "  }\n",
    "\n",
    "def torso_visible(keypoints):\n",
    "  \"\"\"Checks whether there are enough torso keypoints.\n",
    "\n",
    "  This function checks whether the model is confident at predicting one of the\n",
    "  shoulders/hips which is required to determine a good crop region.\n",
    "  \"\"\"\n",
    "  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE or\n",
    "          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE) and\n",
    "          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE or\n",
    "          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE))\n",
    "\n",
    "def determine_torso_and_body_range(\n",
    "    keypoints, target_keypoints, center_y, center_x):\n",
    "  \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
    "\n",
    "  The function returns the maximum distances from the two sets of keypoints:\n",
    "  full 17 keypoints and 4 torso keypoints. The returned information will be\n",
    "  used to determine the crop size. See determineCropRegion for more detail.\n",
    "  \"\"\"\n",
    "  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
    "  max_torso_yrange = 0.0\n",
    "  max_torso_xrange = 0.0\n",
    "  for joint in torso_joints:\n",
    "    dist_y = abs(center_y - target_keypoints[joint][0])\n",
    "    dist_x = abs(center_x - target_keypoints[joint][1])\n",
    "    if dist_y > max_torso_yrange:\n",
    "      max_torso_yrange = dist_y\n",
    "    if dist_x > max_torso_xrange:\n",
    "      max_torso_xrange = dist_x\n",
    "\n",
    "  max_body_yrange = 0.0\n",
    "  max_body_xrange = 0.0\n",
    "  for joint in KEYPOINT_DICT.keys():\n",
    "    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
    "      continue\n",
    "    dist_y = abs(center_y - target_keypoints[joint][0]);\n",
    "    dist_x = abs(center_x - target_keypoints[joint][1]);\n",
    "    if dist_y > max_body_yrange:\n",
    "      max_body_yrange = dist_y\n",
    "\n",
    "    if dist_x > max_body_xrange:\n",
    "      max_body_xrange = dist_x\n",
    "\n",
    "  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
    "\n",
    "def determine_crop_region(\n",
    "      keypoints, image_height,\n",
    "      image_width):\n",
    "  \"\"\"Determines the region to crop the image for the model to run inference on.\n",
    "\n",
    "  The algorithm uses the detected joints from the previous frame to estimate\n",
    "  the square region that encloses the full body of the target person and\n",
    "  centers at the midpoint of two hip joints. The crop size is determined by\n",
    "  the distances between each joints and the center point.\n",
    "  When the model is not confident with the four torso joint predictions, the\n",
    "  function returns a default crop which is the full image padded to square.\n",
    "  \"\"\"\n",
    "  target_keypoints = {}\n",
    "  for joint in KEYPOINT_DICT.keys():\n",
    "    target_keypoints[joint] = [\n",
    "      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
    "      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
    "    ]\n",
    "\n",
    "  if torso_visible(keypoints):\n",
    "    center_y = (target_keypoints['left_hip'][0] +\n",
    "                target_keypoints['right_hip'][0]) / 2;\n",
    "    center_x = (target_keypoints['left_hip'][1] +\n",
    "                target_keypoints['right_hip'][1]) / 2;\n",
    "\n",
    "    (max_torso_yrange, max_torso_xrange,\n",
    "      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
    "          keypoints, target_keypoints, center_y, center_x)\n",
    "\n",
    "    crop_length_half = np.amax(\n",
    "        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
    "          max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
    "\n",
    "    tmp = np.array(\n",
    "        [center_x, image_width - center_x, center_y, image_height - center_y])\n",
    "    crop_length_half = np.amin(\n",
    "        [crop_length_half, np.amax(tmp)]);\n",
    "\n",
    "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
    "\n",
    "    if crop_length_half > max(image_width, image_height) / 2:\n",
    "      return init_crop_region(image_height, image_width)\n",
    "    else:\n",
    "      crop_length = crop_length_half * 2;\n",
    "      return {\n",
    "        'y_min': crop_corner[0] / image_height,\n",
    "        'x_min': crop_corner[1] / image_width,\n",
    "        'y_max': (crop_corner[0] + crop_length) / image_height,\n",
    "        'x_max': (crop_corner[1] + crop_length) / image_width,\n",
    "        'height': (crop_corner[0] + crop_length) / image_height -\n",
    "            crop_corner[0] / image_height,\n",
    "        'width': (crop_corner[1] + crop_length) / image_width -\n",
    "            crop_corner[1] / image_width\n",
    "      }\n",
    "  else:\n",
    "    return init_crop_region(image_height, image_width)\n",
    "\n",
    "def crop_and_resize(image, crop_region, crop_size):\n",
    "  \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
    "  boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
    "          crop_region['y_max'], crop_region['x_max']]]\n",
    "  output_image = tf.image.crop_and_resize(\n",
    "      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
    "  return output_image\n",
    "\n",
    "def run_inference(movenet, image, crop_region, crop_size):\n",
    "  \"\"\"Runs model inference on the cropped region.\n",
    "\n",
    "  The function runs the model inference on the cropped region and updates the\n",
    "  model output to the original image coordinate system.\n",
    "  \"\"\"\n",
    "  image_height, image_width, _ = image.shape\n",
    "  input_image = crop_and_resize(\n",
    "    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
    "  # Run model inference.\n",
    "  keypoints_with_scores = movenet(input_image)\n",
    "  # Update the coordinates.\n",
    "  for idx in range(17):\n",
    "    keypoints_with_scores[0, 0, idx, 0] = (\n",
    "        crop_region['y_min'] * image_height +\n",
    "        crop_region['height'] * image_height *\n",
    "        keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
    "    keypoints_with_scores[0, 0, idx, 1] = (\n",
    "        crop_region['x_min'] * image_width +\n",
    "        crop_region['width'] * image_width *\n",
    "        keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
    "  return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input video.\n",
    "video_path = 'ADL.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "\n",
    "# Read the video frame by frame\n",
    "frames = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # Convert the frame from BGR to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frames.append(frame)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the List of Frames to a 4-D Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the frames to a 4-D tensor\n",
    "frames_np = np.array(frames)\n",
    "image = tf.convert_to_tensor(frames_np, dtype=tf.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bounding_box(image, keypoints_with_scores, threshold=0.3, margin=0.13):\n",
    "    \"\"\"\n",
    "    Add a bounding box to the image based on the keypoints with an additional margin.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The image with keypoints.\n",
    "        keypoints_with_scores (numpy.ndarray): The keypoints with scores.\n",
    "        threshold (float): The confidence threshold to consider a keypoint.\n",
    "        margin (float): The margin to add around the bounding box as a percentage of the box dimensions.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The image with the bounding box.\n",
    "    \"\"\"\n",
    "    # Extract keypoints\n",
    "    keypoints = keypoints_with_scores[0, 0, :, :2]\n",
    "    scores = keypoints_with_scores[0, 0, :, 2]\n",
    "\n",
    "    # Filter keypoints based on the confidence threshold\n",
    "    valid_keypoints = keypoints[scores > threshold]\n",
    "\n",
    "    if valid_keypoints.size == 0:\n",
    "        return image\n",
    "\n",
    "    # Calculate the bounding box coordinates\n",
    "    x_min = np.min(valid_keypoints[:, 1])\n",
    "    y_min = np.min(valid_keypoints[:, 0])\n",
    "    x_max = np.max(valid_keypoints[:, 1])\n",
    "    y_max = np.max(valid_keypoints[:, 0])\n",
    "\n",
    "    # Convert to integer coordinates\n",
    "    x_min = int(x_min * image.shape[1])\n",
    "    y_min = int(y_min * image.shape[0])\n",
    "    x_max = int(x_max * image.shape[1])\n",
    "    y_max = int(y_max * image.shape[0])\n",
    "\n",
    "    # Add margin to the bounding box\n",
    "    box_width = x_max - x_min\n",
    "    box_height = y_max - y_min\n",
    "    x_min = max(0, x_min - int(margin * box_width))\n",
    "    y_min = max(0, y_min - int(margin * box_height))\n",
    "    x_max = min(image.shape[1], x_max + int(margin * box_width))\n",
    "    y_max = min(image.shape[0], y_max + int(margin * box_height))\n",
    "\n",
    "    # Draw the bounding box on the image\n",
    "    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datraframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoints_to_dataframe(keypoints_with_scores, frame_index):\n",
    "    \"\"\"\n",
    "    Converts keypoints with scores to a pandas DataFrame, reorganizes the columns, and removes eye and ear columns.\n",
    "    \n",
    "    Args:\n",
    "      keypoints_with_scores (numpy.ndarray): A numpy array of shape \n",
    "      (1, 1, 17, 3) containing keypoints and their scores. The first \n",
    "      dimension is the batch size, the second dimension is the number \n",
    "      of instances, the third dimension is the number of keypoints \n",
    "      (17 for MoveNet), and the fourth dimension contains the \n",
    "      coordinates (x, y) and the score.\n",
    "      frame_index (int): The index of the frame.\n",
    "    \n",
    "    Returns:\n",
    "      pandas.DataFrame: A DataFrame containing the keypoints' coordinates \n",
    "      with columns named after the keypoint names followed by '_X' \n",
    "      and '_Y' for the x and y coordinates respectively, reorganized \n",
    "      such that x-coordinates come before y-coordinates for each keypoint, \n",
    "      and with eye and ear columns removed.\n",
    "    \"\"\"\n",
    "    keypoints = keypoints_with_scores[0, 0, :, :2]  # Extract keypoints\n",
    "    keypoint_names = [\n",
    "        'Nose', 'Left Eye', 'Right Eye', 'Left Ear', 'Right Ear', 'Left Shoulder', 'Right Shoulder', \n",
    "        'Left Elbow', 'Right Elbow', 'Left Wrist', 'Right Wrist', 'Left Hip', 'Right Hip', \n",
    "        'Left Knee', 'Right Knee', 'Left Ankle', 'Right Ankle'\n",
    "    ]\n",
    "    \n",
    "    # Create column names\n",
    "    columns = []\n",
    "    for name in keypoint_names:\n",
    "        columns.append(f'{name}_Y')\n",
    "        columns.append(f'{name}_X')\n",
    "\n",
    "    # Flatten the keypoints array and create a DataFrame\n",
    "    keypoints_flat = keypoints.flatten()\n",
    "    df = pd.DataFrame([keypoints_flat], columns=columns)\n",
    "    \n",
    "    # Reorganize columns so that x comes before y\n",
    "    x_columns = [col for col in columns if '_X' in col]\n",
    "    y_columns = [col for col in columns if '_Y' in col]\n",
    "    reorganized_columns = []\n",
    "    for x_col, y_col in zip(x_columns, y_columns):\n",
    "        reorganized_columns.append(x_col)\n",
    "        reorganized_columns.append(y_col)\n",
    "    \n",
    "    # # Add frame index column\n",
    "    # df['frame_idx'] = frame_index\n",
    "    \n",
    "    # # Reorder columns to have frame_idx first\n",
    "    # df = df[['frame_idx'] + reorganized_columns]\n",
    "    \n",
    "    # Remove eye and ear columns\n",
    "    columns_to_remove = [\n",
    "        'Left Eye_Y', 'Left Eye_X', \n",
    "        'Right Eye_Y', 'Right Eye_X', \n",
    "        'Left Ear_Y', 'Left Ear_X', \n",
    "        'Right Ear_Y', 'Right Ear_X'\n",
    "    ]\n",
    "    df = df.drop(columns=[col for col in columns_to_remove if col in df.columns])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pose Estimation Inference with Cropping Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def load_video(video_path):\n",
    "    # Load the input video.\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not load video.\")\n",
    "        return\n",
    "\n",
    "    # Read the video frame by frame\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert the frame from BGR to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "    \n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def infer_from_video(frames, movenet, input_size, init_crop_region, run_inference, draw_prediction_on_image, determine_crop_region, confidence_threshold=0.28):\n",
    "    \"\"\"\n",
    "    Perform inference on a video using the MoveNet model and return keypoints as a DataFrame.\n",
    "    Args:\n",
    "      vid_path (str): Path to the input video file.\n",
    "      movenet (tf.Module): The MoveNet model.\n",
    "      input_size (int): The size of the input image for the model.\n",
    "      init_crop_region (function): Function to initialize the crop region.\n",
    "      run_inference (function): Function to run inference on a single frame.\n",
    "      draw_prediction_on_image (function): Function to draw predictions on an image.\n",
    "      determine_crop_region (function): Function to determine the crop region for the next frame.\n",
    "      confidence_threshold (float): Minimum confidence score for keypoints to be included.\n",
    "    Returns:\n",
    "      pd.DataFrame: DataFrame containing keypoints coordinates each frame.\n",
    "    \"\"\"\n",
    "    # # Load the input video.\n",
    "    # cap = cv2.VideoCapture(vid_path)\n",
    "    \n",
    "    # if not cap.isOpened():\n",
    "    #     print(\"Error: Could not load video.\")\n",
    "    #     return\n",
    "\n",
    "    # # Read the video frame by frame\n",
    "    # frames = []\n",
    "    # while True:\n",
    "    #     ret, frame = cap.read()\n",
    "    #     if not ret:\n",
    "    #         break\n",
    "    #     # Convert the frame from BGR to RGB\n",
    "    #     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    #     frames.append(frame)\n",
    "    \n",
    "    # # Release the video capture object\n",
    "    # cap.release()\n",
    "    \n",
    "    # Convert the frames to a 4-D tensor\n",
    "    frames_np = np.array(frames)\n",
    "    image = tf.convert_to_tensor(frames_np, dtype=tf.uint8)\n",
    "\n",
    "    # Load the input image.\n",
    "    num_frames, image_height, image_width, _ = image.shape\n",
    "    crop_region = init_crop_region(image_height, image_width)\n",
    "\n",
    "    # Run the model on the video frames one by one\n",
    "    output_images = []\n",
    "    keypoints_list = []\n",
    "    for frame_idx in range(num_frames):\n",
    "        # Run inference on the frame\n",
    "        keypoints_with_scores = run_inference(\n",
    "            movenet, image[frame_idx, :, :, :], crop_region,\n",
    "            crop_size=[input_size, input_size])\n",
    "\n",
    "        # Set coordinates with confidence scores below the threshold to -1\n",
    "        keypoints_with_scores[0, 0, keypoints_with_scores[0, 0, :, 2] < confidence_threshold, :2] = -1\n",
    "\n",
    "        # Convert keypoints to DataFrame and add frame index\n",
    "        keypoints_df = keypoints_to_dataframe(keypoints_with_scores, frame_idx)\n",
    "        keypoints_list.append(keypoints_df)\n",
    "\n",
    "        # Draw predictions on the frame\n",
    "        output_image = draw_prediction_on_image(\n",
    "            image[frame_idx, :, :, :].numpy().astype(np.int32),\n",
    "            keypoints_with_scores, crop_region=None,\n",
    "            close_figure=True, output_image_height=300)\n",
    "\n",
    "        # Add bounding box to the output image\n",
    "        output_image_with_bbox = add_bounding_box(output_image, keypoints_with_scores)\n",
    "        \n",
    "        # Append the output image to the list\n",
    "        output_images.append(output_image_with_bbox)\n",
    "        \n",
    "        # Determine the crop region for the next frame\n",
    "        crop_region = determine_crop_region(\n",
    "            keypoints_with_scores, image_height, image_width)\n",
    "\n",
    "    # Concatenate all keypoints DataFrames\n",
    "    all_keypoints_df = pd.concat(keypoints_list, ignore_index=True)\n",
    "    return all_keypoints_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def load_video(video_path):\n",
    "    stream = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not stream.isOpened():\n",
    "        print(\"Error: Could not load video.\")\n",
    "        return None\n",
    "    \n",
    "    return stream\n",
    "\n",
    "def display_processed_frame(frame, delay):\n",
    "    cv2.imshow('Processed Frame', frame)\n",
    "    time.sleep(delay / 1000.0)  # Convert delay to seconds\n",
    "\n",
    "    # Check if the user wants to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        return False  # Signal to stop the display\n",
    "    return True  # Continue the display\n",
    "\n",
    "def infer_from_video_in_batches(video_path, movenet, input_size, init_crop_region, run_inference, draw_prediction_on_image, determine_crop_region, confidence_threshold=0.28, batch_size=10):\n",
    "    stream = load_video(video_path)\n",
    "    if stream is None:\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    num_frames = int(stream.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    image_height = int(stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    image_width = int(stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fps = stream.get(cv2.CAP_PROP_FPS)\n",
    "    delay = int(1000 / fps)  # Delay in milliseconds for real-time display\n",
    "    \n",
    "    crop_region = init_crop_region(image_height, image_width)\n",
    "    \n",
    "    keypoints_list = []\n",
    "    frames_batch = []\n",
    "    frame_idx = 0\n",
    "    batch_idx = 0\n",
    "    keep_displaying = True\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = stream.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames_batch.append(frame_rgb)\n",
    "        frame_idx += 1\n",
    "        \n",
    "        # Process batch if we've collected enough frames\n",
    "        if len(frames_batch) == batch_size or frame_idx == num_frames:\n",
    "            # Convert frames batch to a tensor\n",
    "            frames_np = np.array(frames_batch)\n",
    "            frames_tensor = tf.convert_to_tensor(frames_np, dtype=tf.uint8)\n",
    "            \n",
    "            # Process each frame in the batch\n",
    "            for i, frame_rgb in enumerate(frames_batch):\n",
    "                # Run inference on the frame\n",
    "                keypoints_with_scores = run_inference(\n",
    "                    movenet, frames_tensor[i], crop_region, crop_size=[input_size, input_size])\n",
    "                \n",
    "                # Set coordinates with low confidence to -1\n",
    "                keypoints_with_scores[0, 0, keypoints_with_scores[0, 0, :, 2] < confidence_threshold, :2] = -1\n",
    "\n",
    "                # Convert keypoints to DataFrame\n",
    "                keypoints_df = keypoints_to_dataframe(keypoints_with_scores, frame_idx - len(frames_batch) + i)\n",
    "                keypoints_df.insert(0, 'batch_idx', batch_idx)\n",
    "                keypoints_df.insert(1, 'batch_frame', f\"{frame_idx - len(frames_batch)}-{frame_idx - 1}\")\n",
    "                keypoints_list.append(keypoints_df)\n",
    "\n",
    "                # Draw predictions and add bounding box\n",
    "                output_image = draw_prediction_on_image(frame_rgb, keypoints_with_scores)\n",
    "                output_image_with_bbox = add_bounding_box(output_image, keypoints_with_scores)\n",
    "\n",
    "                # Convert frame back to BGR for display\n",
    "                output_image_with_bbox_bgr = cv2.cvtColor(output_image_with_bbox, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # Display the frame using the separate display function\n",
    "                keep_displaying = display_processed_frame(output_image_with_bbox_bgr, delay)\n",
    "\n",
    "                if not keep_displaying:\n",
    "                    break\n",
    "            \n",
    "            # Clear the batch for the next set of frames\n",
    "            frames_batch = []\n",
    "            batch_idx += 1\n",
    "    \n",
    "    # Concatenate all keypoints DataFrames\n",
    "    all_keypoints_df = pd.concat(keypoints_list, ignore_index=True)\n",
    "    return all_keypoints_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def load_video(video_path):\n",
    "    stream = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not stream.isOpened():\n",
    "        print(\"Error: Could not load video.\")\n",
    "        return None\n",
    "    \n",
    "    return stream\n",
    "\n",
    "def display_processed_frame(frame):\n",
    "    \"\"\"\n",
    "    Displays the processed frame as fast as possible, maintaining real-time performance.\n",
    "    \n",
    "    Args:\n",
    "        frame (np.ndarray): The frame to display.\n",
    "    \n",
    "    Returns:\n",
    "        bool: Whether to continue displaying (False if 'q' is pressed).\n",
    "    \"\"\"\n",
    "    cv2.imshow('Processed Frame', frame)\n",
    "\n",
    "    # Check if the user wants to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        return False  # Signal to stop the display\n",
    "    return True  # Continue the display\n",
    "\n",
    "def infer_from_video_in_batches(video_path, movenet, input_size, init_crop_region, run_inference, draw_prediction_on_image, determine_crop_region, confidence_threshold=0.28, batch_size=10):\n",
    "    stream = load_video(video_path)\n",
    "    if stream is None:\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    num_frames = int(stream.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    image_height = int(stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    image_width = int(stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fps = stream.get(cv2.CAP_PROP_FPS)\n",
    "    delay_per_frame = 1000 / fps  # Time per frame in milliseconds for real-time synchronization\n",
    "    \n",
    "    crop_region = init_crop_region(image_height, image_width)\n",
    "    \n",
    "    keypoints_list = []\n",
    "    frames_batch = []\n",
    "    frame_idx = 0\n",
    "    batch_idx = 0\n",
    "    keep_displaying = True\n",
    "    \n",
    "    while True:\n",
    "        start_time = time.time()  # Track when processing of the frame starts\n",
    "\n",
    "        ret, frame = stream.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames_batch.append(frame_rgb)\n",
    "        frame_idx += 1\n",
    "        \n",
    "        # Process batch if we've collected enough frames\n",
    "        if len(frames_batch) == batch_size or frame_idx == num_frames:\n",
    "            # Convert frames batch to a tensor\n",
    "            frames_np = np.array(frames_batch)\n",
    "            frames_tensor = tf.convert_to_tensor(frames_np, dtype=tf.uint8)\n",
    "            \n",
    "            batch_keypoints_list = []\n",
    "            # Process each frame in the batch\n",
    "            for i, frame_rgb in enumerate(frames_batch):\n",
    "                # Run inference on the frame\n",
    "                keypoints_with_scores = run_inference(\n",
    "                    movenet, frames_tensor[i], crop_region, crop_size=[input_size, input_size])\n",
    "                \n",
    "                # Set coordinates with low confidence to -1\n",
    "                keypoints_with_scores[0, 0, keypoints_with_scores[0, 0, :, 2] < confidence_threshold, :2] = -1\n",
    "\n",
    "                # Convert keypoints to DataFrame\n",
    "                keypoints_df = keypoints_to_dataframe(keypoints_with_scores, frame_idx - len(frames_batch) + i)\n",
    "                keypoints_df.insert(0, 'frame_idx', frame_idx - len(frames_batch) + i)\n",
    "                keypoints_df.insert(1, 'batch_idx', batch_idx)\n",
    "                batch_keypoints_list.append(keypoints_df)\n",
    "\n",
    "                # Draw predictions and add bounding box\n",
    "                output_image = draw_prediction_on_image(frame_rgb, keypoints_with_scores)\n",
    "                output_image_with_bbox = add_bounding_box(output_image, keypoints_with_scores)\n",
    "\n",
    "                # Convert frame back to BGR for display\n",
    "                output_image_with_bbox_bgr = cv2.cvtColor(output_image_with_bbox, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # Display the frame using the separate display function\n",
    "                keep_displaying = display_processed_frame(output_image_with_bbox_bgr)\n",
    "\n",
    "                if not keep_displaying:\n",
    "                    break\n",
    "\n",
    "                # Ensure real-time display by accounting for processing time\n",
    "                processing_time = (time.time() - start_time) * 1000  # Processing time in milliseconds\n",
    "                if delay_per_frame > processing_time:\n",
    "                    time.sleep((delay_per_frame - processing_time) / 1000)  # Wait if ahead of schedule\n",
    "            \n",
    "            # Clear the batch for the next set of frames\n",
    "            frames_batch = []\n",
    "            batch_idx += 1\n",
    "\n",
    "            # Concatenate keypoints from the current batch\n",
    "            batch_keypoints_df = pd.concat(batch_keypoints_list, ignore_index=True)\n",
    "\n",
    "            # Output the DataFrame for the batch\n",
    "            yield batch_keypoints_df  # Return the DataFrame for this batch for further processing\n",
    "\n",
    "        if not keep_displaying:\n",
    "            break\n",
    "    \n",
    "    # Release the video capture object\n",
    "    stream.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n",
      "Processed batch with 10 keypoints\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize the MoveNet model and other necessary components\n",
    "video_path = \"ADL.mp4\"\n",
    "for batch_df in infer_from_video_in_batches(video_path, movenet, input_size, init_crop_region, run_inference, draw_prediction_on_image, determine_crop_region):\n",
    "    # Perform further calculations with batch_df\n",
    "    print(f\"Processed batch with {len(batch_df)} keypoints\")\n",
    "    # Example: You could calculate velocity/acceleration, save the batch, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object infer_from_video_in_batches at 0x360292580>\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "MoveNet = infer_from_video_in_batches('ADL.mp4', movenet, input_size, init_crop_region, run_inference, draw_prediction_on_image, determine_crop_region, confidence_threshold=0.28, batch_size=10)\n",
    "print(MoveNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining pose estimated frames into video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output_image in output_images:\n",
    "    cv2.imshow('Processed Frame', cv2.cvtColor(output_image, cv2.COLOR_RGB2BGR))\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i42 Adaption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preping functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in ./MoveNet/lib/python3.9/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./MoveNet/lib/python3.9/site-packages (from opencv-python) (1.26.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/nick/Documents/GitHub/MoveNet/MoveNet/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in ./MoveNet/lib/python3.9/site-packages (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (3.5.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (1.66.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: packaging in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: six>=1.12.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: setuptools in ./MoveNet/lib/python3.9/site-packages (from tensorflow) (56.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./MoveNet/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: namex in ./MoveNet/lib/python3.9/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in ./MoveNet/lib/python3.9/site-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: rich in ./MoveNet/lib/python3.9/site-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./MoveNet/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./MoveNet/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./MoveNet/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./MoveNet/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./MoveNet/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./MoveNet/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./MoveNet/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./MoveNet/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.18,>=2.17->tensorflow) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./MoveNet/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.18,>=2.17->tensorflow) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./MoveNet/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./MoveNet/lib/python3.9/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./MoveNet/lib/python3.9/site-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./MoveNet/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/nick/Documents/GitHub/MoveNet/MoveNet/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow-hub in ./MoveNet/lib/python3.9/site-packages (0.16.1)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in ./MoveNet/lib/python3.9/site-packages (from tensorflow-hub) (4.25.5)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow-hub) (1.26.4)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow-hub) (2.17.0)\n",
      "Requirement already satisfied: tensorflow<2.18,>=2.17 in ./MoveNet/lib/python3.9/site-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.17.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.4.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.32.3)\n",
      "Requirement already satisfied: setuptools in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (56.0.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.4.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
      "Requirement already satisfied: packaging in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (24.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.4.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (1.66.2)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (24.3.25)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.1.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (4.12.2)\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.12.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./MoveNet/lib/python3.9/site-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./MoveNet/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.44.0)\n",
      "Requirement already satisfied: optree in ./MoveNet/lib/python3.9/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.12.1)\n",
      "Requirement already satisfied: rich in ./MoveNet/lib/python3.9/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (13.8.1)\n",
      "Requirement already satisfied: namex in ./MoveNet/lib/python3.9/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./MoveNet/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./MoveNet/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./MoveNet/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./MoveNet/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.10)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./MoveNet/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.0.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./MoveNet/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./MoveNet/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.7)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./MoveNet/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./MoveNet/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./MoveNet/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./MoveNet/lib/python3.9/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./MoveNet/lib/python3.9/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./MoveNet/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow-hub) (0.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/nick/Documents/GitHub/MoveNet/MoveNet/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in ./MoveNet/lib/python3.9/site-packages (1.26.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/nick/Documents/GitHub/MoveNet/MoveNet/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in ./MoveNet/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./MoveNet/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./MoveNet/lib/python3.9/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./MoveNet/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./MoveNet/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./MoveNet/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/nick/Documents/GitHub/MoveNet/MoveNet/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python\n",
    "%pip install tensorflow\n",
    "%pip install tensorflow-hub\n",
    "%pip install numpy\n",
    "%pip install pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Import matplotlib libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.25):\n",
    "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "  Args:\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    height: height of the image in pixels.\n",
    "    width: width of the image in pixels.\n",
    "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "      visualized.\n",
    "\n",
    "  Returns:\n",
    "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "      * the coordinates of all keypoints of all detected entities;\n",
    "      * the coordinates of all skeleton edges of all detected entities;\n",
    "      * the colors in which the edges should be plotted.\n",
    "  \"\"\"\n",
    "  keypoints_all = []\n",
    "  keypoint_edges_all = []\n",
    "  edge_colors = []\n",
    "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "  for idx in range(num_instances):\n",
    "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "    kpts_absolute_xy = np.stack(\n",
    "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "        kpts_scores > keypoint_threshold, :]\n",
    "    keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "        keypoint_edges_all.append(line_seg)\n",
    "        edge_colors.append(color)\n",
    "  if keypoints_all:\n",
    "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "  else:\n",
    "    keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "  if keypoint_edges_all:\n",
    "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "  else:\n",
    "    edges_xy = np.zeros((0, 2, 2))\n",
    "  return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "  \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "  Args:\n",
    "    image: A numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "      of the crop region in normalized coordinates (see the init_crop_region\n",
    "      function below for more detail). If provided, this function will also\n",
    "      draw the bounding box on the image.\n",
    "    output_image_height: An integer indicating the height of the output image.\n",
    "      Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "  \"\"\"\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  # To remove the huge white borders\n",
    "  fig.tight_layout(pad=0)\n",
    "  ax.margins(0)\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_xticklabels([])\n",
    "  plt.axis('off')\n",
    "\n",
    "  im = ax.imshow(image)\n",
    "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "  ax.add_collection(line_segments)\n",
    "  # Turn off tick labels\n",
    "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "  (keypoint_locs, keypoint_edges,\n",
    "   edge_colors) = _keypoints_and_edges_for_display(\n",
    "       keypoints_with_scores, height, width)\n",
    "\n",
    "  line_segments.set_segments(keypoint_edges)\n",
    "  line_segments.set_color(edge_colors)\n",
    "  if keypoint_edges.shape[0]:\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "  if keypoint_locs.shape[0]:\n",
    "    scat.set_offsets(keypoint_locs)\n",
    "\n",
    "  if crop_region is not None:\n",
    "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "    rect = patches.Rectangle(\n",
    "        (xmin,ymin),rec_width,rec_height,\n",
    "        linewidth=1,edgecolor='b',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "  fig.canvas.draw()\n",
    "  image_from_plot = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)\n",
    "  image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
    "  plt.close(fig)\n",
    "  if output_image_height is not None:\n",
    "    output_image_width = int(output_image_height / height * width)\n",
    "    image_from_plot = cv2.resize(\n",
    "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "        interpolation=cv2.INTER_CUBIC)\n",
    "  return image_from_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.25):\n",
    "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "  Args:\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    height: height of the image in pixels.\n",
    "    width: width of the image in pixels.\n",
    "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "      visualized.\n",
    "\n",
    "  Returns:\n",
    "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "      * the coordinates of all keypoints of all detected entities;\n",
    "      * the coordinates of all skeleton edges of all detected entities;\n",
    "      * the colors in which the edges should be plotted.\n",
    "  \"\"\"\n",
    "  keypoints_all = []\n",
    "  keypoint_edges_all = []\n",
    "  edge_colors = []\n",
    "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "  for idx in range(num_instances):\n",
    "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "    kpts_absolute_xy = np.stack(\n",
    "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "        kpts_scores > keypoint_threshold, :]\n",
    "    keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "        keypoint_edges_all.append(line_seg)\n",
    "        edge_colors.append(color)\n",
    "  if keypoints_all:\n",
    "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "  else:\n",
    "    keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "  if keypoint_edges_all:\n",
    "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "  else:\n",
    "    edges_xy = np.zeros((0, 2, 2))\n",
    "  return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "def draw_prediction_on_image(image, keypoints_with_scores, crop_region=None, close_figure=False, output_image_height=None):\n",
    "    \"\"\"\n",
    "    Draws keypoints, edges, and bounding box on an image and returns the resulting image.\n",
    "    \n",
    "    Args:\n",
    "        image (np.ndarray): The input image on which to draw.\n",
    "        keypoints_with_scores (np.ndarray): Array containing keypoints and their scores.\n",
    "        crop_region (dict, optional): Dictionary specifying the crop region with keys 'x_min', 'y_min', 'x_max', and 'y_max'. Defaults to None.\n",
    "        close_figure (bool, optional): Whether to close the figure after drawing. Defaults to False.\n",
    "        output_image_height (int, optional): Desired height of the output image. If specified, the output image will be resized to this height while maintaining the aspect ratio. Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The image with keypoints, edges, and bounding box drawn on it.\n",
    "    \"\"\"\n",
    "    height, width, channel = image.shape\n",
    "    aspect_ratio = float(width) / height\n",
    "    fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "    # To remove the huge white borders\n",
    "    fig.tight_layout(pad=0)\n",
    "    ax.margins(0)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    plt.axis('off')\n",
    "\n",
    "    im = ax.imshow(image)\n",
    "    line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "    ax.add_collection(line_segments)\n",
    "    # Turn off tick labels\n",
    "    scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "    (keypoint_locs, keypoint_edges, edge_colors) = _keypoints_and_edges_for_display(\n",
    "        keypoints_with_scores, height, width)\n",
    "\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "    if keypoint_edges.shape[0]:\n",
    "        line_segments.set_segments(keypoint_edges)\n",
    "        line_segments.set_color(edge_colors)\n",
    "    if keypoint_locs.shape[0]:\n",
    "        scat.set_offsets(keypoint_locs)\n",
    "\n",
    "    if crop_region is not None:\n",
    "        xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "        ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "        rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "        rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "        rect = patches.Rectangle(\n",
    "            (xmin, ymin), rec_width, rec_height,\n",
    "            linewidth=1, edgecolor='b', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    image_from_plot = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)\n",
    "    image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
    "    plt.close(fig)\n",
    "    if output_image_height is not None:\n",
    "        output_image_width = int(output_image_height / height * width)\n",
    "        image_from_plot = cv2.resize(\n",
    "            image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "            interpolation=cv2.INTER_CUBIC)\n",
    "    return image_from_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading MoveNet - Single Pose Thunder 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 19:25:02.260586: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-10-02 19:25:02.260609: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-10-02 19:25:02.260636: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-10-02 19:25:02.260660: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-10-02 19:25:02.260674: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Load the MoveNet model from TensorFlow Hub\n",
    "try:\n",
    "    module = hub.load(\"https://www.kaggle.com/models/google/movenet/TensorFlow2/singlepose-thunder/4\")\n",
    "    input_size = 256\n",
    "except Exception as error:\n",
    "    print(f\"Failed to load the model: {error}\")\n",
    "\n",
    "def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "        input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "        A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "        coordinates and scores.\n",
    "    \"\"\"\n",
    "    model = module.signatures['serving_default']\n",
    "\n",
    "    # SavedModel format expects tensor type of int32.\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    # Run model inference.\n",
    "    outputs = model(input_image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoints_with_scores = outputs['output_0'].numpy()\n",
    "    \n",
    "    return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live Stream (Image Sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cropping Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence score to determine whether a keypoint prediction is reliable.\n",
    "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
    "\n",
    "def init_crop_region(image_height, image_width):\n",
    "  \"\"\"Defines the default crop region.\n",
    "\n",
    "  The function provides the initial crop region (pads the full image from both\n",
    "  sides to make it a square image) when the algorithm cannot reliably determine\n",
    "  the crop region from the previous frame.\n",
    "  \"\"\"\n",
    "  if image_width > image_height:\n",
    "    box_height = image_width / image_height\n",
    "    box_width = 1.0\n",
    "    y_min = (image_height / 2 - image_width / 2) / image_height\n",
    "    x_min = 0.0\n",
    "  else:\n",
    "    box_height = 1.0\n",
    "    box_width = image_height / image_width\n",
    "    y_min = 0.0\n",
    "    x_min = (image_width / 2 - image_height / 2) / image_width\n",
    "\n",
    "  return {\n",
    "    'y_min': y_min,\n",
    "    'x_min': x_min,\n",
    "    'y_max': y_min + box_height,\n",
    "    'x_max': x_min + box_width,\n",
    "    'height': box_height,\n",
    "    'width': box_width\n",
    "  }\n",
    "\n",
    "def torso_visible(keypoints):\n",
    "  \"\"\"Checks whether there are enough torso keypoints.\n",
    "\n",
    "  This function checks whether the model is confident at predicting one of the\n",
    "  shoulders/hips which is required to determine a good crop region.\n",
    "  \"\"\"\n",
    "  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE or\n",
    "          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE) and\n",
    "          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE or\n",
    "          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE))\n",
    "\n",
    "def determine_torso_and_body_range(\n",
    "    keypoints, target_keypoints, center_y, center_x):\n",
    "  \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
    "\n",
    "  The function returns the maximum distances from the two sets of keypoints:\n",
    "  full 17 keypoints and 4 torso keypoints. The returned information will be\n",
    "  used to determine the crop size. See determineCropRegion for more detail.\n",
    "  \"\"\"\n",
    "  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
    "  max_torso_yrange = 0.0\n",
    "  max_torso_xrange = 0.0\n",
    "  for joint in torso_joints:\n",
    "    dist_y = abs(center_y - target_keypoints[joint][0])\n",
    "    dist_x = abs(center_x - target_keypoints[joint][1])\n",
    "    if dist_y > max_torso_yrange:\n",
    "      max_torso_yrange = dist_y\n",
    "    if dist_x > max_torso_xrange:\n",
    "      max_torso_xrange = dist_x\n",
    "\n",
    "  max_body_yrange = 0.0\n",
    "  max_body_xrange = 0.0\n",
    "  for joint in KEYPOINT_DICT.keys():\n",
    "    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
    "      continue\n",
    "    dist_y = abs(center_y - target_keypoints[joint][0]);\n",
    "    dist_x = abs(center_x - target_keypoints[joint][1]);\n",
    "    if dist_y > max_body_yrange:\n",
    "      max_body_yrange = dist_y\n",
    "\n",
    "    if dist_x > max_body_xrange:\n",
    "      max_body_xrange = dist_x\n",
    "\n",
    "  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
    "\n",
    "def determine_crop_region(\n",
    "      keypoints, image_height,\n",
    "      image_width):\n",
    "  \"\"\"Determines the region to crop the image for the model to run inference on.\n",
    "\n",
    "  The algorithm uses the detected joints from the previous frame to estimate\n",
    "  the square region that encloses the full body of the target person and\n",
    "  centers at the midpoint of two hip joints. The crop size is determined by\n",
    "  the distances between each joints and the center point.\n",
    "  When the model is not confident with the four torso joint predictions, the\n",
    "  function returns a default crop which is the full image padded to square.\n",
    "  \"\"\"\n",
    "  target_keypoints = {}\n",
    "  for joint in KEYPOINT_DICT.keys():\n",
    "    target_keypoints[joint] = [\n",
    "      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
    "      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
    "    ]\n",
    "\n",
    "  if torso_visible(keypoints):\n",
    "    center_y = (target_keypoints['left_hip'][0] +\n",
    "                target_keypoints['right_hip'][0]) / 2;\n",
    "    center_x = (target_keypoints['left_hip'][1] +\n",
    "                target_keypoints['right_hip'][1]) / 2;\n",
    "\n",
    "    (max_torso_yrange, max_torso_xrange,\n",
    "      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
    "          keypoints, target_keypoints, center_y, center_x)\n",
    "\n",
    "    crop_length_half = np.amax(\n",
    "        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
    "          max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
    "\n",
    "    tmp = np.array(\n",
    "        [center_x, image_width - center_x, center_y, image_height - center_y])\n",
    "    crop_length_half = np.amin(\n",
    "        [crop_length_half, np.amax(tmp)]);\n",
    "\n",
    "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
    "\n",
    "    if crop_length_half > max(image_width, image_height) / 2:\n",
    "      return init_crop_region(image_height, image_width)\n",
    "    else:\n",
    "      crop_length = crop_length_half * 2;\n",
    "      return {\n",
    "        'y_min': crop_corner[0] / image_height,\n",
    "        'x_min': crop_corner[1] / image_width,\n",
    "        'y_max': (crop_corner[0] + crop_length) / image_height,\n",
    "        'x_max': (crop_corner[1] + crop_length) / image_width,\n",
    "        'height': (crop_corner[0] + crop_length) / image_height -\n",
    "            crop_corner[0] / image_height,\n",
    "        'width': (crop_corner[1] + crop_length) / image_width -\n",
    "            crop_corner[1] / image_width\n",
    "      }\n",
    "  else:\n",
    "    return init_crop_region(image_height, image_width)\n",
    "\n",
    "def crop_and_resize(image, crop_region, crop_size):\n",
    "  \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
    "  boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
    "          crop_region['y_max'], crop_region['x_max']]]\n",
    "  output_image = tf.image.crop_and_resize(\n",
    "      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
    "  return output_image\n",
    "\n",
    "def run_inference(movenet, image, crop_region, crop_size):\n",
    "  \"\"\"Runs model inference on the cropped region.\n",
    "\n",
    "  The function runs the model inference on the cropped region and updates the\n",
    "  model output to the original image coordinate system.\n",
    "  \"\"\"\n",
    "  image_height, image_width, _ = image.shape\n",
    "  input_image = crop_and_resize(\n",
    "    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
    "  # Run model inference.\n",
    "  keypoints_with_scores = movenet(input_image)\n",
    "  # Update the coordinates.\n",
    "  for idx in range(17):\n",
    "    keypoints_with_scores[0, 0, idx, 0] = (\n",
    "        crop_region['y_min'] * image_height +\n",
    "        crop_region['height'] * image_height *\n",
    "        keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
    "    keypoints_with_scores[0, 0, idx, 1] = (\n",
    "        crop_region['x_min'] * image_width +\n",
    "        crop_region['width'] * image_width *\n",
    "        keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
    "  return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the keypoints into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoints_to_dataframe(keypoints_with_scores):\n",
    "  \"\"\"\n",
    "  Converts keypoints with scores to a pandas DataFrame, reorganizes the columns, and removes eye and ear columns.\n",
    "  \n",
    "  Args:\n",
    "    keypoints_with_scores (numpy.ndarray): A numpy array of shape \n",
    "    (1, 1, 17, 3) containing keypoints and their scores. The first \n",
    "    dimension is the batch size, the second dimension is the number \n",
    "    of instances, the third dimension is the number of keypoints \n",
    "    (17 for MoveNet), and the fourth dimension contains the \n",
    "    coordinates (x, y) and the score.\n",
    "  \n",
    "  Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the keypoints' coordinates \n",
    "    with columns named after the keypoint names followed by '_X' \n",
    "    and '_Y' for the x and y coordinates respectively, reorganized \n",
    "    such that x-coordinates come before y-coordinates for each keypoint, \n",
    "    and with eye and ear columns removed.\n",
    "  \"\"\"\n",
    "  keypoints = keypoints_with_scores[0, 0, :, :2]  # Extract keypoints\n",
    "  keypoint_names = [\n",
    "    'Nose', 'Left Eye', 'Right Eye', 'Left Ear', 'Right Ear', 'Left Shoulder', 'Right Shoulder', \n",
    "    'Left Elbow', 'Right Elbow', 'Left Wrist', 'Right Wrist', 'Left Hip', 'Right Hip', \n",
    "    'Left Knee', 'Right Knee', 'Left Ankle', 'Right Ankle'\n",
    "  ]\n",
    "  \n",
    "  # Create column names\n",
    "  columns = []\n",
    "  for name in keypoint_names:\n",
    "    columns.append(f'{name}_Y')\n",
    "    columns.append(f'{name}_X')\n",
    "\n",
    "  # Flatten the keypoints array and create a DataFrame\n",
    "  keypoints_flat = keypoints.flatten()\n",
    "  df = pd.DataFrame([keypoints_flat], columns=columns)\n",
    "  \n",
    "  # Reorganize columns so that x comes before y\n",
    "  x_columns = [col for col in columns if '_X' in col]\n",
    "  y_columns = [col for col in columns if '_Y' in col]\n",
    "  reorganized_columns = []\n",
    "  for x_col, y_col in zip(x_columns, y_columns):\n",
    "    reorganized_columns.append(x_col)\n",
    "    reorganized_columns.append(y_col)\n",
    "  df = df[reorganized_columns]\n",
    "  \n",
    "  # Remove eye and ear columns\n",
    "  columns_to_remove = [\n",
    "    'Left Eye_Y', 'Left Eye_X', \n",
    "    'Right Eye_Y', 'Right Eye_X', \n",
    "    'Left Ear_Y', 'Left Ear_X', \n",
    "    'Right Ear_Y', 'Right Ear_X'\n",
    "  ]\n",
    "  df = df.drop(columns=columns_to_remove)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference on Live Video Stream Frame from OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bounding_box(image, keypoints_with_scores, threshold=0.4, margin=0.2):\n",
    "    \"\"\"\n",
    "    Add a bounding box to the image based on the keypoints with an additional margin.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The image with keypoints.\n",
    "        keypoints_with_scores (numpy.ndarray): The keypoints with scores.\n",
    "        threshold (float): The confidence threshold to consider a keypoint.\n",
    "        margin (float): The margin to add around the bounding box as a percentage of the box dimensions.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The image with the bounding box.\n",
    "    \"\"\"\n",
    "    # Extract keypoints\n",
    "    keypoints = keypoints_with_scores[0, 0, :, :2]\n",
    "    scores = keypoints_with_scores[0, 0, :, 2]\n",
    "\n",
    "    # Filter keypoints based on the confidence threshold\n",
    "    valid_keypoints = keypoints[scores > threshold]\n",
    "\n",
    "    if valid_keypoints.size == 0:\n",
    "        return image\n",
    "\n",
    "    # Calculate the bounding box coordinates\n",
    "    x_min = np.min(valid_keypoints[:, 1])\n",
    "    y_min = np.min(valid_keypoints[:, 0])\n",
    "    x_max = np.max(valid_keypoints[:, 1])\n",
    "    y_max = np.max(valid_keypoints[:, 0])\n",
    "\n",
    "    # Convert to integer coordinates\n",
    "    x_min = int(x_min * image.shape[1])\n",
    "    y_min = int(y_min * image.shape[0])\n",
    "    x_max = int(x_max * image.shape[1])\n",
    "    y_max = int(y_max * image.shape[0])\n",
    "\n",
    "    # Add margin to the bounding box\n",
    "    box_width = x_max - x_min\n",
    "    box_height = y_max - y_min\n",
    "    x_min = max(0, x_min - int(margin * box_width))\n",
    "    y_min = max(0, y_min - int(margin * box_height))\n",
    "    x_max = min(image.shape[1], x_max + int(margin * box_width))\n",
    "    y_max = min(image.shape[0], y_max + int(margin * box_height))\n",
    "\n",
    "    # Draw the bounding box on the image\n",
    "    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def stream_and_infer_from_camera(movenet, input_size, init_crop_region, run_inference, draw_prediction_on_image, determine_crop_region):\n",
    "    \"\"\"\n",
    "    Capture live stream from the system camera and run inference on the captured frames.\n",
    "\n",
    "    Args:\n",
    "        movenet: The pose estimation model.\n",
    "        input_size: The input size for the model.\n",
    "        init_crop_region: Function to initialize the crop region.\n",
    "        run_inference: Function to run inference on a frame.\n",
    "        draw_prediction_on_image: Function to draw predictions on a frame.\n",
    "        determine_crop_region: Function to determine the crop region based on keypoints.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Open the system camera\n",
    "    stream = cv2.VideoCapture('0')\n",
    "\n",
    "    if not stream.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "\n",
    "    # Initialize crop region\n",
    "    ret, frame = stream.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame from camera.\")\n",
    "        return\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_height, image_width, _ = frame.shape\n",
    "    crop_region = init_crop_region(image_height, image_width)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = stream.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame from BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = tf.convert_to_tensor(frame_rgb, dtype=tf.uint8)\n",
    "        frame_tensor = tf.expand_dims(frame_tensor, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Run inference\n",
    "        keypoints_with_scores = run_inference(\n",
    "            movenet, frame_tensor[0], crop_region,\n",
    "            crop_size=[input_size, input_size])\n",
    "        \n",
    "        # Draw predictions on the frame\n",
    "        output_frame = draw_prediction_on_image(\n",
    "            frame_rgb.astype(np.int32),\n",
    "            keypoints_with_scores, crop_region=None,\n",
    "            close_figure=True, output_image_height=300)\n",
    "        \n",
    "        # Update crop region\n",
    "        crop_region = determine_crop_region(\n",
    "            keypoints_with_scores, image_height, image_width)\n",
    "        \n",
    "        # Add bounding box to the frame\n",
    "        output_frame = add_bounding_box(output_frame, keypoints_with_scores)\n",
    "\n",
    "        # Display the live stream with predictions\n",
    "        cv2.imshow('Live Stream', cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR))\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    stream.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MoveNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
